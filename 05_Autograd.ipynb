{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_Autograd.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Automatic Differentiation with `torch.autograd`\n"
      ],
      "metadata": {
        "id": "KrfQQ4FjoHV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tensors, Functions and Computational Graph\n",
        "- In below, `w` and `b` are parameters which we need to optimize.\n",
        "- To compute the gradients of loss function with respect to those variables, we set the `requires_grad=True` of those tensors."
      ],
      "metadata": {
        "id": "U4x-My8Wo6Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.ones(5)\n",
        "y = torch.zeros(3)\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w) + b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
      ],
      "metadata": {
        "id": "gz0RDGoWobsK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function that we apply to tensors to construct computational graph is in fact an object of class `Function`. This object knows how to compute the function in the forward direction, and also how to compute its derivative during the backward propagation step. A reference to the backward propagation function is stored in `grad_fn` property of a tensor."
      ],
      "metadata": {
        "id": "RhRqK-X3p7Pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d43ExQ3fp5qH",
        "outputId": "eb0992a8-0aad-4bea-a864-ce4de072b418"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7f6749ba4710>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7f6749bee390>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Computing Gradients\n"
      ],
      "metadata": {
        "id": "ibHyNSdop4So"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)\n",
        "print(z.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b04uVQiSrQfT",
        "outputId": "df271f6c-27ff-47c8-a87f-c22b0db051a6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1952, 0.1672, 0.3291],\n",
            "        [0.1952, 0.1672, 0.3291],\n",
            "        [0.1952, 0.1672, 0.3291],\n",
            "        [0.1952, 0.1672, 0.3291],\n",
            "        [0.1952, 0.1672, 0.3291]])\n",
            "tensor([0.1952, 0.1672, 0.3291])\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Disabling Gradient Tracking\n",
        "- By default, all tensors with `requried_grad=True` are tracking their computational history and support gradient computation.\n",
        "- However, we only want to do forward computations through the network.\n",
        "- We can stop tracking computations by surrounding our computation code with `torch.no_grad()` block."
      ],
      "metadata": {
        "id": "fh9eVRDhsCQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w) + b\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OywxgGDfrWS0",
        "outputId": "9b5fc057-01ef-481c-fba3-6974bbb4617d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.matmul(x, w) + b\n",
        "z_det = z.detach()\n",
        "print(z_det.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AXoZC0Lsj8J",
        "outputId": "461a72db-6e80-4894-d1dd-265c4ad181ca"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    }
  ]
}